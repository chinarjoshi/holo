#+TITLE: Project


* Planning
There are a few ways to implement the image proccessing

** AutoCad PNGOUT
_See how fast autocad can output 4 images of the same obj file ong_

Now we don't need a raspberry pi or arduino, instead everything will be done from the host computer.

The webcam will be utilized from the host computer, and controls will be given from the keyboard.
Press G to enable gesture control for the laptop, V to start visualization, < to decrease size and > to increase size.

Fitting the model to be flat on the bottom would be a bit difficult to calculate though

Factors at play:
 - Cost
 - Durability
 - Feature-rich
 - Functionality

For live cadding, we have to either:
 1. Take 4 rotated screenshots at a time.
 2. Split pane view with 1 screenshot and opencv processing.

    switch to tinkerkcad and get rid of the background.

** Python libraries
There are certain python libraries i can use to import the obj files.
 - pywavefront:
   * Pros:
     - Seems easy enough to load in obj files and visualize them
   * Cons:
     Restricted to one image taker in pyglet

 - pyopengl:
   * Pros:
     - Actual opengl library made for 3d rendering job
   * Cons:
     - May end up being resource intensive

* Software Specs
There are two distinct and equally important aspects of the project
** Live Cadding Tool
Integrates into your cadding software

** Pre-exported wavefront rendering
Use PyOpenGL or PyMesh in order to load the file into the mesh and take distinct screenshots from all sides.

What i see now is that i have to use wavefront and pyglet in order to render the objects

* Library

There are two options for the back-bone libraries i can use for model rendering
** OpenGL
OpenGL sucks pp but you can use it in conjunction with SDL if you need an interface with it.

** Blender Python API
Only use this if you're able to remove the background and grid lines, and render multiple images on different offsets from the center of the screen.

* TODO
"""Creates blender window that interacts with gesture recognition software.

Sadly every single api call for blender has side effects so it is very difficult
to manage the current state. Here is what should be implemented

 0. Gesture recognition runs upon the click of a button through multithreading
 1. A new window is spawned to be the size of the monitor we have
 2. Using bpy.data, we split the viewbpy.


1. bpy.data.window_managers[0].windows[1].screen.areas[0].spaces[0].region_3d.view_camera_offset
2. bpy.data.objects['Cube'].rotation_euler = Euler((20, 20, 20))

ok so by manipulating bpy.data.object['Cube'], i can efficetly influence all 4 areas of the
view. That is the only part i have to update, the object orientation and rotation. On the
other hand, scale and rotation of the camera is handled by the window manager. On the window
manager i have to offset area 1 view by 90 degrees

so in essesnce i have to manipulate the window manager and the object, both found in
bpy.data
"""
